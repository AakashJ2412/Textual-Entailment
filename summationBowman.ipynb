{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yss/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#IMPORT LIBRARIES, CAN PROBABLY BE MORE CLEAN\n",
    "\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, merge, Dropout, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import L2\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTION FOR READING DATA\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'entailment': 0, 'contradiction': 1, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING TRAIN DATA\n",
    "\n",
    "fileName = 'snli_1.0_train.txt'\n",
    "\n",
    "with open(fileName, 'r') as f:\n",
    "    trainRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "trainPremises = [extract(row[1]) for row in trainRows if row[0] in labels]\n",
    "trainHypotheses = [extract(row[2]) for row in trainRows if row[0] in labels]\n",
    "trainLabels = [labels[row[0]] for row in trainRows if row[0] in labels]\n",
    "\n",
    "trainData = [trainPremises, trainHypotheses, trainLabels]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING TEST DATA\n",
    "\n",
    "fileName = 'snli_1.0_test.txt'\n",
    "\n",
    "with open(fileName, 'r') as f:\n",
    "    testRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "testPremises = [extract(row[1]) for row in testRows if row[0] in labels]\n",
    "testHypotheses = [extract(row[2]) for row in testRows if row[0] in labels]\n",
    "testLabels = [labels[row[0]] for row in testRows if row[0] in labels]\n",
    "\n",
    "testData = [testPremises, testHypotheses, testLabels]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "maxLen = 50\n",
    "epochs = 10\n",
    "batchSize = 256\n",
    "gloveDimension = 100\n",
    "regularization = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER TO CREATE VOCABULARY\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
    "vocabSize = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TRAIN DATA TO SEQUENCES AS PER VOCABULARY\n",
    "#PAD OR TRIM ALL SENTENCES TO SAME LENGTH\n",
    "#CONVERT LABELS TO ONE HOT ENCODING\n",
    "\n",
    "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
    "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
    "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
    "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
    "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT TEST DATA TO SEQUENCES AS PER VOCABULARY\n",
    "#PAD OR TRIM ALL SENTENCES TO SAME LENGTH\n",
    "#CONVERT LABELS TO ONE HOT ENCODING\n",
    "\n",
    "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
    "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
    "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
    "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
    "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GLOVE FILE AND STORE ALL EMBEDDINGS\n",
    "#NEEDS TO BE RUN ONLY ONCE\n",
    "#MAKE SURE TO ADD THE CORRECT FILE NAME FOR GLOVE\n",
    "\n",
    "embeddingsDict = dict()\n",
    "glove = open(\"/home/yss/Documents/sem6/nlp/glove.6B.100d.txt\")\n",
    "\n",
    "for line in glove:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddingsDict[word] = vector_dimensions\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM THE EMBEDDINGS, STORE ALL THE ONES THAT ARE IN OUR VOCABULARY\n",
    "\n",
    "embeddingsMat = zeros((vocabSize, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vec = embeddingsDict.get(word)\n",
    "    if vec is not None:\n",
    "        embeddingsMat[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE EMBEDDING LAYER FOR MODEL\n",
    "#SINCE MODEL IS NOT SEQUENTIAL AND DEPENDS ON TWO SEPERATE INPUTS,\n",
    "#DEFINE TWO INPUTS AND EMBED\n",
    "\n",
    "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
    "\n",
    "premise = Input(shape=(maxLen,), dtype='int32')\n",
    "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
    "\n",
    "premInput = embed(premise)\n",
    "hypoInput = embed(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AS PER THE PAPER, THIS IS THE FIRST TECHNIQUE\n",
    "#ONCE THE EMBEDDINGS OF A SENTENCE IS THERE, WHAT YOU HAVE IS A\n",
    "#MATRIX OF maxLen X gloveDimension. ADD ALONG maxLen TO GET\n",
    "#A SINGLE EMBEDDING VECTOR OF LENGTH gloveDimension\n",
    "\n",
    "rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=gloveDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT ENTIRELY SURE WHAT BATCH NORMALIZATION DOES\n",
    "#BUT IT WAS IN THE SOURCE I REFERRED TO\n",
    "#ALTHOUGH I DIDN'T SEE IT IN THE PAPER\n",
    "\n",
    "premInput = rnn(premInput)\n",
    "hypoInput = rnn(hypoInput)\n",
    "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
    "hypoInput = tf.keras.layers.BatchNormalization()(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOINT IS THE CONCATENATED LAYER OF THE SUMMED PREMISE AND HYPOTHESIS\n",
    "#DROPOUTS WERE NOT MENTIONED IN THE PAPER, BUT IS PROBABLY A STANDARD PRACTICE\n",
    "#THE PAPER HAS 3 LAYERS OF TANH ALONG WITH SOME L2 REGULARIZATION\n",
    "#AND FINALLY THE DECISION IS BASED ON SOFTMAX\n",
    "\n",
    "\n",
    "joint = keras.layers.concatenate([premInput, hypoInput])\n",
    "joint = Dropout(0.2)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(200, activation='tanh', kernel_regularizer=L2(regularization))(joint)\n",
    "    joint = Dropout(0.2)(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization()(joint)\n",
    "\n",
    "pred = Dense(3, activation='softmax')(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING MODEL INPUT AND OUTPUT AND COMPILATION\n",
    "\n",
    "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2146/2146 [==============================] - 17s 7ms/step - loss: 0.9950 - accuracy: 0.5153\n",
      "Epoch 2/10\n",
      "2146/2146 [==============================] - 18s 8ms/step - loss: 0.9243 - accuracy: 0.5666\n",
      "Epoch 3/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.9004 - accuracy: 0.5838\n",
      "Epoch 4/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8872 - accuracy: 0.5930\n",
      "Epoch 5/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8794 - accuracy: 0.5985\n",
      "Epoch 6/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8742 - accuracy: 0.6027\n",
      "Epoch 7/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8688 - accuracy: 0.6055\n",
      "Epoch 8/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8658 - accuracy: 0.6082\n",
      "Epoch 9/10\n",
      "2146/2146 [==============================] - 19s 9ms/step - loss: 0.8629 - accuracy: 0.6110\n",
      "Epoch 10/10\n",
      "2146/2146 [==============================] - 20s 9ms/step - loss: 0.8613 - accuracy: 0.6109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2915af3e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FITTING THE MODEL TO TRAIN DATA\n",
    "\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 4ms/step - loss: 0.7860 - accuracy: 0.6607\n",
      "Loss =  0.7859595417976379\n",
      "Acc =  0.6607288122177124\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
