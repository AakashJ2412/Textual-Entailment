{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/raghav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.regularizers import L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Input, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that cleans the input data and enumerates the labels\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'ENTAILMENT': 0, 'CONTRADICTION': 1, 'NEUTRAL': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads data and parses data from file\n",
    "\n",
    "fileName = 'SICK.txt'\n",
    "\n",
    "with open(fileName, 'r') as f:\n",
    "    Rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "trainRows = [row for row in Rows if row[11]=='TRAIN\\n']\n",
    "testRows = [row for row in Rows if row[11]=='TEST\\n']\n",
    "validationRows = [row for row in Rows if row[11]=='TRIAL\\n']\n",
    "\n",
    "trainPremises = [extract(row[1]) for row in trainRows if row[3] in labels]\n",
    "trainHypotheses = [extract(row[2]) for row in trainRows if row[3] in labels]\n",
    "trainLabels = [labels[row[3]] for row in trainRows if row[3] in labels]\n",
    "\n",
    "trainData = [trainPremises, trainHypotheses, trainLabels]\n",
    "\n",
    "testPremises = [extract(row[1]) for row in testRows if row[3] in labels]\n",
    "testHypotheses = [extract(row[2]) for row in testRows if row[3] in labels]\n",
    "testLabels = [labels[row[3]] for row in testRows if row[3] in labels]\n",
    "\n",
    "testData = [testPremises, testHypotheses, testLabels]\n",
    "\n",
    "validationPremises = [extract(row[1]) for row in validationRows if row[3] in labels]\n",
    "validationHypotheses = [extract(row[2]) for row in validationRows if row[3] in labels]\n",
    "validationLabels = [labels[row[3]] for row in validationRows if row[3] in labels]\n",
    "\n",
    "validationData = [validationPremises, validationHypotheses, validationLabels]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "maxLen = 32\n",
    "epochs = 1000\n",
    "batchSize = 128\n",
    "gloveDimension = 300\n",
    "hiddenDimension = 100\n",
    "regularization = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to generate the vocabulary of the system\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
    "vocabSize = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train data to sequences as per the vocabulary\n",
    "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
    "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
    "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test data to sequences as per the vocabulary\n",
    "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
    "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
    "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the validation data to sequences as per the vocabulary\n",
    "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
    "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
    "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsDict = dict()\n",
    "glove = open(r'glove.840B.300d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove:\n",
    "    records = line.split()\n",
    "    word = ''.join(records[:-300])\n",
    "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
    "    embeddingsDict[word] = vectorDimensions\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
    "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vec = embeddingsDict.get(word)\n",
    "    if vec is not None:\n",
    "        embeddingsMat[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 14:48:44.400691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-30 14:48:44.412547: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.412955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.413164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.413371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.413537: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.413691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.414000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.414262: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-30 14:48:44.414315: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-30 14:48:44.418384: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding layer for our baseline RNN model\n",
    "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
    "\n",
    "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
    "premise = Input(shape=(maxLen,), dtype='int32')\n",
    "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
    "\n",
    "premInput = embed(premise)\n",
    "hypoInput = embed(hypothesis)\n",
    "\n",
    "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
    "\n",
    "premInput = convert(premInput)\n",
    "hypoInput = convert(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
    "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
    "\n",
    "rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
    "#rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply batch normalization to the two input embeddings separately\n",
    "\n",
    "premInput = rnn(premInput)\n",
    "hypoInput = rnn(hypoInput)\n",
    "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
    "hypoInput = tf.keras.layers.BatchNormalization()(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
    "# Dilution of probability 0.2, to assist in regularization\n",
    "joint = keras.layers.concatenate([premInput, hypoInput])\n",
    "joint = Dropout(0.2)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(joint)\n",
    "    joint = Dropout(0.2)(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization()(joint)\n",
    "\n",
    "# 3 layers of the TanH activation function, along with L2 regularization.\n",
    "# The final decision is based on the Softmax function\n",
    "pred = Dense(3, activation='softmax')(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final models input and output format, as well as compilation parameters\n",
    "\n",
    "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.3494 - accuracy: 0.8585 - val_loss: 1.2253 - val_accuracy: 0.6202\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.3473 - accuracy: 0.8635 - val_loss: 1.2093 - val_accuracy: 0.6364\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.3194 - accuracy: 0.8698 - val_loss: 1.3352 - val_accuracy: 0.6303\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3032 - accuracy: 0.8786 - val_loss: 0.9330 - val_accuracy: 0.7131\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 0.3200 - accuracy: 0.8702 - val_loss: 1.3688 - val_accuracy: 0.6081\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3252 - accuracy: 0.8689 - val_loss: 1.1743 - val_accuracy: 0.6606\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 0.3221 - accuracy: 0.8714 - val_loss: 1.1365 - val_accuracy: 0.6606\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 0.3008 - accuracy: 0.8817 - val_loss: 1.1172 - val_accuracy: 0.6768\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.3106 - accuracy: 0.8784 - val_loss: 1.0613 - val_accuracy: 0.6747\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3126 - accuracy: 0.8718 - val_loss: 1.1233 - val_accuracy: 0.6687\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.3184 - accuracy: 0.8770 - val_loss: 1.2157 - val_accuracy: 0.6646\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.3090 - accuracy: 0.8761 - val_loss: 1.4375 - val_accuracy: 0.6202\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.3122 - accuracy: 0.8743 - val_loss: 1.2250 - val_accuracy: 0.6646\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.2877 - accuracy: 0.8874 - val_loss: 0.9815 - val_accuracy: 0.6949\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 0.2856 - accuracy: 0.8851 - val_loss: 1.0677 - val_accuracy: 0.6848\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.2895 - accuracy: 0.8849 - val_loss: 1.0871 - val_accuracy: 0.6828\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.2958 - accuracy: 0.8765 - val_loss: 1.1781 - val_accuracy: 0.6869\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 0.2892 - accuracy: 0.8808 - val_loss: 1.0870 - val_accuracy: 0.6747\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.2883 - accuracy: 0.8876 - val_loss: 1.0842 - val_accuracy: 0.6909\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.2794 - accuracy: 0.8889 - val_loss: 1.0921 - val_accuracy: 0.6869\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2754 - accuracy: 0.8865 - val_loss: 1.1857 - val_accuracy: 0.6545\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.2806 - accuracy: 0.8930 - val_loss: 1.0993 - val_accuracy: 0.6869\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.2788 - accuracy: 0.8912 - val_loss: 1.2063 - val_accuracy: 0.6586\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2781 - accuracy: 0.8867 - val_loss: 1.0110 - val_accuracy: 0.7192\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 1s 129ms/step - loss: 0.2603 - accuracy: 0.8993 - val_loss: 1.1632 - val_accuracy: 0.6808\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2579 - accuracy: 0.9020 - val_loss: 1.2497 - val_accuracy: 0.6768\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2668 - accuracy: 0.8910 - val_loss: 1.1061 - val_accuracy: 0.6990\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.2671 - accuracy: 0.8955 - val_loss: 0.9298 - val_accuracy: 0.7273\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.2620 - accuracy: 0.8973 - val_loss: 1.2641 - val_accuracy: 0.6727\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.2595 - accuracy: 0.8934 - val_loss: 1.0086 - val_accuracy: 0.7111\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2600 - accuracy: 0.8939 - val_loss: 0.9978 - val_accuracy: 0.7192\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.2612 - accuracy: 0.8966 - val_loss: 1.0637 - val_accuracy: 0.7192\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.2454 - accuracy: 0.9031 - val_loss: 0.9772 - val_accuracy: 0.7232\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.2421 - accuracy: 0.9056 - val_loss: 0.9858 - val_accuracy: 0.7253\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.2475 - accuracy: 0.9099 - val_loss: 1.1984 - val_accuracy: 0.6929\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2316 - accuracy: 0.9099 - val_loss: 1.1722 - val_accuracy: 0.7051\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2405 - accuracy: 0.9067 - val_loss: 1.1350 - val_accuracy: 0.7010\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2448 - accuracy: 0.9040 - val_loss: 1.2556 - val_accuracy: 0.6727\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2282 - accuracy: 0.9112 - val_loss: 1.0550 - val_accuracy: 0.7152\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2349 - accuracy: 0.9072 - val_loss: 1.0092 - val_accuracy: 0.7152\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 0.2100 - accuracy: 0.9189 - val_loss: 1.2167 - val_accuracy: 0.6889\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.2312 - accuracy: 0.9166 - val_loss: 1.0914 - val_accuracy: 0.7212\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2433 - accuracy: 0.9043 - val_loss: 1.2961 - val_accuracy: 0.6626\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 0.2201 - accuracy: 0.9155 - val_loss: 1.1060 - val_accuracy: 0.6949\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.2329 - accuracy: 0.9088 - val_loss: 0.9869 - val_accuracy: 0.7192\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2146 - accuracy: 0.9189 - val_loss: 1.1602 - val_accuracy: 0.7010\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2179 - accuracy: 0.9146 - val_loss: 1.0380 - val_accuracy: 0.7030\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.2163 - accuracy: 0.9182 - val_loss: 1.0123 - val_accuracy: 0.7273\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.2180 - accuracy: 0.9126 - val_loss: 1.0613 - val_accuracy: 0.7131\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2100 - accuracy: 0.9171 - val_loss: 1.0041 - val_accuracy: 0.7253\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2011 - accuracy: 0.9227 - val_loss: 1.1557 - val_accuracy: 0.6990\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.2146 - accuracy: 0.9187 - val_loss: 1.3217 - val_accuracy: 0.6667\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.2208 - accuracy: 0.9166 - val_loss: 1.1890 - val_accuracy: 0.7071\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.2165 - accuracy: 0.9182 - val_loss: 1.0102 - val_accuracy: 0.7152\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2081 - accuracy: 0.9207 - val_loss: 0.9929 - val_accuracy: 0.7394\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.2091 - accuracy: 0.9203 - val_loss: 1.3659 - val_accuracy: 0.6525\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 0.2019 - accuracy: 0.9218 - val_loss: 1.2394 - val_accuracy: 0.6949\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.2100 - accuracy: 0.9200 - val_loss: 1.2628 - val_accuracy: 0.6727\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.1892 - accuracy: 0.9304 - val_loss: 0.9805 - val_accuracy: 0.7273\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.2053 - accuracy: 0.9198 - val_loss: 1.4662 - val_accuracy: 0.6626\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 1s 124ms/step - loss: 0.1812 - accuracy: 0.9317 - val_loss: 1.0329 - val_accuracy: 0.7374\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 1s 122ms/step - loss: 0.2006 - accuracy: 0.9236 - val_loss: 1.1588 - val_accuracy: 0.7010\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.2022 - accuracy: 0.9261 - val_loss: 1.1507 - val_accuracy: 0.6990\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.1882 - accuracy: 0.9306 - val_loss: 1.0906 - val_accuracy: 0.7212\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.1985 - accuracy: 0.9243 - val_loss: 1.1640 - val_accuracy: 0.7212\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.1866 - accuracy: 0.9322 - val_loss: 1.1849 - val_accuracy: 0.6909\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.1969 - accuracy: 0.9216 - val_loss: 1.0223 - val_accuracy: 0.7232\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.1886 - accuracy: 0.9257 - val_loss: 1.2973 - val_accuracy: 0.6889\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.1903 - accuracy: 0.9302 - val_loss: 1.1940 - val_accuracy: 0.7010\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 1s 126ms/step - loss: 0.1824 - accuracy: 0.9338 - val_loss: 1.2467 - val_accuracy: 0.7030\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.1763 - accuracy: 0.9299 - val_loss: 1.1087 - val_accuracy: 0.7111\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 0.1853 - accuracy: 0.9308 - val_loss: 1.2146 - val_accuracy: 0.6970\n",
      "Epoch 73/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.1921 - accuracy: 0.9225 - val_loss: 1.0840 - val_accuracy: 0.7192\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.1827 - accuracy: 0.9315 - val_loss: 1.0595 - val_accuracy: 0.7131\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.1862 - accuracy: 0.9313 - val_loss: 1.1932 - val_accuracy: 0.7111\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.1838 - accuracy: 0.9331 - val_loss: 1.1075 - val_accuracy: 0.7212\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 1s 120ms/step - loss: 0.1826 - accuracy: 0.9306 - val_loss: 1.0530 - val_accuracy: 0.7273\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.1813 - accuracy: 0.9331 - val_loss: 1.0481 - val_accuracy: 0.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4495250d30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=50)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=512, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 29ms/step - loss: 1.1644 - accuracy: 0.7085\n",
      "Loss =  1.16437566280365\n",
      "Acc =  0.7085201740264893\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./lstm50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./lstm50/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7faeb37c78e0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('./summ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 109ms/step - loss: 1.2887 - accuracy: 0.7265\n",
      "0.726457417011261\n"
     ]
    }
   ],
   "source": [
    "ccc = keras.models.load_model('./lstm50')\n",
    "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
