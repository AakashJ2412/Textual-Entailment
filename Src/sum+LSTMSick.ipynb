{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/raghav/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.regularizers import L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Input, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that cleans the input data and enumerates the labels\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'ENTAILMENT': 0, 'CONTRADICTION': 1, 'NEUTRAL': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads data and parses data from file\n",
    "\n",
    "fileName = 'SICK.txt'\n",
    "\n",
    "with open(fileName, 'r') as f:\n",
    "    Rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "trainRows = [row for row in Rows if row[11]=='TRAIN\\n']\n",
    "testRows = [row for row in Rows if row[11]=='TEST\\n']\n",
    "validationRows = [row for row in Rows if row[11]=='TRIAL\\n']\n",
    "\n",
    "trainPremises = [extract(row[1]) for row in trainRows if row[3] in labels]\n",
    "trainHypotheses = [extract(row[2]) for row in trainRows if row[3] in labels]\n",
    "trainLabels = [labels[row[3]] for row in trainRows if row[3] in labels]\n",
    "\n",
    "trainData = [trainPremises, trainHypotheses, trainLabels]\n",
    "\n",
    "testPremises = [extract(row[1]) for row in testRows if row[3] in labels]\n",
    "testHypotheses = [extract(row[2]) for row in testRows if row[3] in labels]\n",
    "testLabels = [labels[row[3]] for row in testRows if row[3] in labels]\n",
    "\n",
    "testData = [testPremises, testHypotheses, testLabels]\n",
    "\n",
    "validationPremises = [extract(row[1]) for row in validationRows if row[3] in labels]\n",
    "validationHypotheses = [extract(row[2]) for row in validationRows if row[3] in labels]\n",
    "validationLabels = [labels[row[3]] for row in validationRows if row[3] in labels]\n",
    "\n",
    "validationData = [validationPremises, validationHypotheses, validationLabels]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "maxLen = 50\n",
    "epochs = 1000\n",
    "batchSize = 128\n",
    "gloveDimension = 300\n",
    "hiddenDimension = 100\n",
    "regularization = 4e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer to generate the vocabulary of the system\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainData[0] + trainData[1])\n",
    "vocabSize = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train data to sequences as per the vocabulary\n",
    "trainData[0] = tokenizer.texts_to_sequences(trainData[0])\n",
    "trainData[1] = tokenizer.texts_to_sequences(trainData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "trainData[0] = pad_sequences(trainData[0], maxLen, padding='post')\n",
    "trainData[1] = pad_sequences(trainData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "trainData[2] = tf.keras.utils.to_categorical(trainData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test data to sequences as per the vocabulary\n",
    "testData[0] = tokenizer.texts_to_sequences(testData[0])\n",
    "testData[1] = tokenizer.texts_to_sequences(testData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "testData[0] = pad_sequences(testData[0], maxLen, padding='post')\n",
    "testData[1] = pad_sequences(testData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "testData[2] = tf.keras.utils.to_categorical(testData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the validation data to sequences as per the vocabulary\n",
    "validationData[0] = tokenizer.texts_to_sequences(validationData[0])\n",
    "validationData[1] = tokenizer.texts_to_sequences(validationData[1])\n",
    "\n",
    "# Pad or trim all generated sequences to the same max sentence length\n",
    "validationData[0] = pad_sequences(validationData[0], maxLen, padding='post')\n",
    "validationData[1] = pad_sequences(validationData[1], maxLen, padding='post')\n",
    "\n",
    "# Transform the labels to one-hot encoding\n",
    "validationData[2] = tf.keras.utils.to_categorical(validationData[2], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsDict = dict()\n",
    "glove = open(r'glove.840B.300d.txt', encoding='utf8')\n",
    "\n",
    "for line in glove:\n",
    "    records = line.split()\n",
    "    word = ''.join(records[:-300])\n",
    "    vectorDimensions = asarray(records[-300:], dtype='float32')\n",
    "    embeddingsDict[word] = vectorDimensions\n",
    "\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the embeddings and store only those that are present in our vocabulary\n",
    "embeddingsMat = zeros((vocabSize, gloveDimension))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    vec = embeddingsDict.get(word)\n",
    "    if vec is not None:\n",
    "        embeddingsMat[index] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding layer for our baseline RNN model\n",
    "embed = Embedding(vocabSize, gloveDimension, weights=[embeddingsMat], input_length=maxLen, trainable=False)\n",
    "\n",
    "# As Premise and Hypothesis are distinct and are to be inputted separately, define two inputs and embed\n",
    "premise = Input(shape=(maxLen,), dtype='int32')\n",
    "hypothesis = Input(shape=(maxLen,), dtype='int32')\n",
    "\n",
    "premInput = embed(premise)\n",
    "hypoInput = embed(hypothesis)\n",
    "\n",
    "convert = Dense(hiddenDimension, activation='tanh', input_shape=(gloveDimension,))\n",
    "\n",
    "premInput = convert(premInput)\n",
    "hypoInput = convert(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the sentence embeddings have been generated, generate a matrix of dimensions maxLen X gloveDimension\n",
    "# On adding maxLen, we get a single embedding vector of length gloveDimension\n",
    "\n",
    "#rnn = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=hiddenDimension)\n",
    "rnn = LSTM(hiddenDimension, dropout=0.2, recurrent_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply batch normalization to the two input embeddings separately\n",
    "\n",
    "premInput = rnn(premInput)\n",
    "hypoInput = rnn(hypoInput)\n",
    "premInput = tf.keras.layers.BatchNormalization()(premInput)\n",
    "hypoInput = tf.keras.layers.BatchNormalization()(hypoInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint is a concatenated embeddings layer, generated from the premise and hypothesis inputs\n",
    "# Dilution of probability 0.2, to assist in regularization\n",
    "joint = keras.layers.concatenate([premInput, hypoInput])\n",
    "joint = Dropout(0.2)(joint)\n",
    "for i in range(3):\n",
    "    joint = Dense(2*hiddenDimension, activation='tanh', kernel_regularizer=L2(regularization))(joint)\n",
    "    joint = Dropout(0.2)(joint)\n",
    "    joint = tf.keras.layers.BatchNormalization()(joint)\n",
    "\n",
    "# 3 layers of the TanH activation function, along with L2 regularization.\n",
    "# The final decision is based on the Softmax function\n",
    "pred = Dense(3, activation='softmax')(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final models input and output format, as well as compilation parameters\n",
    "\n",
    "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "35/35 [==============================] - 23s 378ms/step - loss: 1.3364 - accuracy: 0.3814 - val_loss: 0.9708 - val_accuracy: 0.5677\n",
      "Epoch 2/1000\n",
      "35/35 [==============================] - 12s 354ms/step - loss: 1.1264 - accuracy: 0.4620 - val_loss: 0.9629 - val_accuracy: 0.5677\n",
      "Epoch 3/1000\n",
      "35/35 [==============================] - 12s 344ms/step - loss: 1.0936 - accuracy: 0.4877 - val_loss: 0.9608 - val_accuracy: 0.5677\n",
      "Epoch 4/1000\n",
      "35/35 [==============================] - 12s 344ms/step - loss: 1.0646 - accuracy: 0.5071 - val_loss: 0.9681 - val_accuracy: 0.5677\n",
      "Epoch 5/1000\n",
      "35/35 [==============================] - 12s 346ms/step - loss: 1.0156 - accuracy: 0.5157 - val_loss: 0.9680 - val_accuracy: 0.5798\n",
      "Epoch 6/1000\n",
      "35/35 [==============================] - 12s 341ms/step - loss: 0.9380 - accuracy: 0.5265 - val_loss: 0.9340 - val_accuracy: 0.5657\n",
      "Epoch 7/1000\n",
      "35/35 [==============================] - 12s 347ms/step - loss: 0.9162 - accuracy: 0.5335 - val_loss: 1.0016 - val_accuracy: 0.3495\n",
      "Epoch 8/1000\n",
      "35/35 [==============================] - 12s 340ms/step - loss: 0.8524 - accuracy: 0.5589 - val_loss: 0.7872 - val_accuracy: 0.6182\n",
      "Epoch 9/1000\n",
      "35/35 [==============================] - 12s 344ms/step - loss: 0.8202 - accuracy: 0.5648 - val_loss: 0.7788 - val_accuracy: 0.5778\n",
      "Epoch 10/1000\n",
      "35/35 [==============================] - 12s 347ms/step - loss: 0.8090 - accuracy: 0.5718 - val_loss: 0.7699 - val_accuracy: 0.6040\n",
      "Epoch 11/1000\n",
      "35/35 [==============================] - 13s 361ms/step - loss: 0.7898 - accuracy: 0.5715 - val_loss: 0.8115 - val_accuracy: 0.5253\n",
      "Epoch 12/1000\n",
      "35/35 [==============================] - 12s 352ms/step - loss: 0.7845 - accuracy: 0.5688 - val_loss: 0.7527 - val_accuracy: 0.6101\n",
      "Epoch 13/1000\n",
      "35/35 [==============================] - 12s 348ms/step - loss: 0.7857 - accuracy: 0.5731 - val_loss: 0.7394 - val_accuracy: 0.6121\n",
      "Epoch 14/1000\n",
      "35/35 [==============================] - 12s 344ms/step - loss: 0.8054 - accuracy: 0.5837 - val_loss: 0.8145 - val_accuracy: 0.5616\n",
      "Epoch 15/1000\n",
      "35/35 [==============================] - 12s 350ms/step - loss: 0.8102 - accuracy: 0.5731 - val_loss: 0.8573 - val_accuracy: 0.4424\n",
      "Epoch 16/1000\n",
      "35/35 [==============================] - 12s 351ms/step - loss: 0.7733 - accuracy: 0.5848 - val_loss: 0.7934 - val_accuracy: 0.5475\n",
      "Epoch 17/1000\n",
      "35/35 [==============================] - 12s 343ms/step - loss: 0.7699 - accuracy: 0.5792 - val_loss: 0.7845 - val_accuracy: 0.5758\n",
      "Epoch 18/1000\n",
      "35/35 [==============================] - 12s 329ms/step - loss: 0.7535 - accuracy: 0.5868 - val_loss: 0.7503 - val_accuracy: 0.5535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88c77abbb0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model using the train data\n",
    "\n",
    "callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "model.fit([array(trainData[0]), array(trainData[1])], array(trainData[2]), batch_size=batchSize, epochs=epochs, callbacks=[callback], validation_data=[[array(validationData[0]), array(validationData[1])], array(validationData[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 3s 161ms/step - loss: 0.7478 - accuracy: 0.5628\n",
      "Loss =  0.7478418946266174\n",
      "Acc =  0.5627802610397339\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy on the trained model\n",
    "\n",
    "loss, acc = model.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print('Loss = ', loss)\n",
    "print('Acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./lstm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f472e431cd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('./lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 4s 78ms/step - loss: 0.5769 - accuracy: 0.7730\n",
      "0.7730048894882202\n"
     ]
    }
   ],
   "source": [
    "ccc = keras.models.load_model('./lstm')\n",
    "l, a = ccc.evaluate([array(testData[0]), array(testData[1])], array(testData[2]), batch_size=256)\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
