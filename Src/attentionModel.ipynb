{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that cleans the input data and enumerates the labels\n",
    "\n",
    "def extract(s):\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "labels = {'entailment': 0, 'contradiction': 1, 'neutral': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads data and parses data from file\n",
    "\n",
    "def readFileData(filePath):\n",
    "    with open(filePath, 'r') as f:\n",
    "        inputRows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "\n",
    "    inputPremises = [extract(row[1]) for row in inputRows if row[0] in labels]\n",
    "    inputHypotheses = [extract(row[2]) for row in inputRows if row[0] in labels]\n",
    "    inputLabels = [labels[row[0]] for row in inputRows if row[0] in labels]\n",
    "    f.close()\n",
    "\n",
    "    return [inputPremises, inputHypotheses, inputLabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train and test data\n",
    "\n",
    "trainData = readFileData(trainFilePath)\n",
    "testData = readFileData(testFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base parameters\n",
    "\n",
    "sentenceLength = 50\n",
    "batchSize = 256\n",
    "numWorkers = d2l.get_dataloader_workers()\n",
    "embedSize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base DataSet class that initializes and tokenizes the input data, using the d2l tokenize function, and pytorch tensors\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, sentenceLength, vocab=None):\n",
    "        self.sentenceLength = sentenceLength\n",
    "        premiseTokens = d2l.tokenize(dataset[0])\n",
    "        hypothesisTokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(premiseTokens + hypothesisTokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.premises = self._pad(premiseTokens)\n",
    "        self.hypotheses = self._pad(hypothesisTokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([d2l.truncate_pad(self.vocab[line], self.sentenceLength, self.vocab['<pad>']) for line in lines])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.premises)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the class for our training and testing set\n",
    "\n",
    "trainSet = DataSet(trainData, sentenceLength)\n",
    "vocab = trainSet.vocab\n",
    "\n",
    "testSet = DataSet(testData, sentenceLength, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batch data from the training/testing set via the pytorch data loader\n",
    "\n",
    "trainIter = torch.utils.data.DataLoader(trainSet, batchSize, shuffle=True, num_workers=numWorkers)\n",
    "testIter = torch.utils.data.DataLoader(testSet, batchSize, shuffle=False, num_workers=numWorkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the multi-layer perceptron, with dropout probability 0.2\n",
    "def mlp(numInputs, numHiddens, flatten):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(numInputs, numHiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(numHiddens, numHiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attend stage of attention model\n",
    "class Attend(nn.Module):\n",
    "    def __init__(self, numInputs, numHiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(numInputs, numHiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        fA = self.f(A)\n",
    "        fB = self.f(B)\n",
    "        \n",
    "        e = torch.bmm(fA, fB.permute(0, 2, 1))\n",
    "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stage of attention model\n",
    "class Compare(nn.Module):\n",
    "    def __init__(self, numInputs, numHiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(numInputs, numHiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        VA = self.g(torch.cat([A, beta], dim=2))\n",
    "        VB = self.g(torch.cat([B, alpha], dim=2))\n",
    "        return VA, VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate stage of attention model\n",
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, numInputs, numHiddens, numOutputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(numInputs, numHiddens, flatten=True)\n",
    "        self.linear = nn.Linear(numHiddens, numOutputs)\n",
    "\n",
    "    def forward(self, VA, VB):\n",
    "        VA = VA.sum(dim=1)\n",
    "        VB = VB.sum(dim=1)\n",
    "        yHat = self.linear(self.h(torch.cat([VA, VB], dim=1)))\n",
    "        return yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver class for entailment classifier\n",
    "class NLI(nn.Module):\n",
    "    def __init__(self, vocab, embedSize, numHiddens, numInputsAttend=100,\n",
    "                 numInputsCompare=200, numInputsAggregate=400, **kwargs):\n",
    "        super(NLI, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embedSize)\n",
    "        self.attend = Attend(numInputsAttend, numHiddens)\n",
    "        self.compare = Compare(numInputsCompare, numHiddens)\n",
    "        self.aggregate = Aggregate(numInputsAggregate, numHiddens, numOutputs=3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        VA, VB = self.compare(A, B, beta, alpha)\n",
    "        yHat = self.aggregate(VA, VB)\n",
    "        return yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GloVe embeddings and activate multithreading/GPU acceleration\n",
    "\n",
    "numHiddens, devices = 200, d2l.try_all_gpus()\n",
    "model = NLI(vocab, embedSize, numHiddens)\n",
    "gloveEmbedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = gloveEmbedding[vocab.idx_to_token]\n",
    "model.embedding.weight.data.copy_(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate and number of epochs, and train the model\n",
    "\n",
    "learningRate, epochs = 0.001, 4\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(model, trainIter, testIter, loss, trainer, epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict function to test premise and hypothesis\n",
    "def predict(model, vocab, premise, hypothesis):\n",
    "    model.eval()\n",
    "    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())\n",
    "    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\n",
    "    label = torch.argmax(model([premise.reshape((1, -1)),\n",
    "                            hypothesis.reshape((1, -1))]), dim=1)\n",
    "\n",
    "    if label == 0:\n",
    "        return 'entailment'\n",
    "    elif label == 1:\n",
    "        return 'contradiction'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
